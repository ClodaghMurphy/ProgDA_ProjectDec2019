{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2019 Programming for Data Analysis\n",
    "\n",
    "* Choose a real-world phenomenon that can be measured and for which you could collect at least one-hundred data points across at least four different variables.<br>\n",
    "* Investigate the types of variables involved, their likely distributions, and their relationships with each other<br>\n",
    "* Synthesise/simulate a data set as closely matching their properties as possible.<br>\n",
    "* Detail your research and implement the simulation in a Jupyter notebook – the data set itself can simply be displayed in an output cell within the notebook.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose a real-world phenomenon that can be measured and for which you could collect at least one-hundred data points across at least four different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I have selected a dataset that is available from the Irish Government's open data project to research, investigate then simulate some of the variables.<br>\n",
    "The Open Data project is an initiative by the government of Ireland that makes data held by public bodies available and easily accessible online for reuse and redistribution to create interest and encourage engagement with open data.<br>\n",
    "I have chosen the [Office of Public Works Heritage Site Details](https://www.opw.ie/en/media/opw-heritage-site-details.csv) open dataset which contains one hundred data points across twenty-four variables and was collected in 2015.<br>\n",
    "The Office of Public Works (OPW) is a Government department with responsibility for the day-to-day running of all National Monuments in State care and National Historic Properties. <br>\n",
    "\n",
    "The real-world phenomenon that is presented is a collection of information relating to the Heritage Sites that are open to the public.  <br>\n",
    "I chose this dataset because it is of interest to me in my professional life. <br>\n",
    "In the next section of the project I will explore the kinds of variables that appear in a dataset relating to Heritage Sites, the relationships (if any) between variables and the distributions that are apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate the types of variables involved, their likely distributions, and their relationships with each other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Investigation of the original OPW dataset**\n",
    "\n",
    "In order to simulate a dataset on the subject of Heritage States owned by the State/ citizens of Ireland I must investigate a pre-existing one.[Office of Public Works Heritage Site Details](https://www.opw.ie/en/media/opw-heritage-site-details.csv)<br>\n",
    "\n",
    "**Observations on The Types of Variables**\n",
    "\n",
    "The dataset contains information about 100 unique, named Heritage Sites managed by The Office of Public Works collected in 2015.<br>\n",
    "There are 24 different variables in the original dataset, most of which relate to visitor information e.g. GPS co-ordinates and contact details for the site.\n",
    "The following points are relevant to this exercise and the objective of synthesising data set in a methodical way which can match the contents.\n",
    "\n",
    "#### Heritage Site Name\n",
    "* Every Heritage Site name is a unique object\n",
    "\n",
    "#### Pricing structures in Euro Datatype: Integer\n",
    "* Adult\tentrance price - an integer between 0 and 12 \n",
    "* Senior / Group entrance price\t- an integer between 0 and 9 \n",
    "* Child entrance price - an integer between 0 and 7 \n",
    "* Student entrance price - an integer between 0 and 8 \n",
    "* Family entrance price - an integer between 0 and 32 \n",
    "\n",
    "* 51% of the sites have free admission, 35% have an adult entrance fee of €5.\n",
    "* When an entrance fee is paid, there is a price point for all types of visitors.\n",
    "* An individual adult is the most expensive ticket with all others reducing by 1 or 2 euro from that point\n",
    "* A family ticket is approximately the same price as the sum of two adult plus one child tickets\n",
    "\n",
    "#### Visitor Numbers Datatype : Integer\n",
    "* 2015 Visitor Numbers contain integers that range from 0 - 553,348. As previously state there is a strong relationship between the Region and Visitor Numbers.\n",
    "\n",
    "* 31 of the entries for 2015 Visitor Numbers contain a null value.\n",
    "* The remaining 69 datapoints show that visitor numbers range from 1750 to 553348\n",
    "* The total number of visitors is 5.1 million people\n",
    "\n",
    "\n",
    "#### Geographical Location Datatype : Object\n",
    "* The county where the Heritage Site is located affects the Regional classification, if this information were to be shuffled, the county/region need to be linked.\n",
    "* There is no relationship between the number of Heritage Sites in a county and the visitor numbers.\n",
    "* There is a strong relationship between the Region and the Visitor Numbers.\n",
    "* Instead of joining the county and region, I will therefore omit the county variable entirely my reason for this is because the county variable has no strong relationship with any distribution - the regionality is a stronger one.\n",
    "\n",
    "Heritage Sites are in 7 different geographical regions, \n",
    "* Dublin\n",
    "* Midlands & East Coast \n",
    "* North-West\n",
    "* Shannon\n",
    "* South-East\n",
    "* South-West\n",
    "* West\n",
    "\n",
    "The majority of sites are located in Dublin, South - East and South-West.\n",
    "\n",
    "#### Cafe facilities Datatype : Integer\n",
    "In the original dataset, 9 out of 100 Heritage Sites have a Cafe on site\n",
    "\n",
    "#### Opening Dates Datatype : Object (Yes/No)\n",
    "42 of the sites are open all year round, the remainder have seasonal opening times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likely Distributions in the OPW Heritage Sites Dataset used to inform a synthesised Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationships in the OPW Heritage Sites Dataset used to inform a synthesised Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3\n",
    "# Synthesise/simulate a data set as closely matching the properties of the original as is possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following on from the findings in Section 2, I will simulate a data set as closely matching their properties as possible using the numpy random package thus building on my previous work carried out during this course where I explored the numpy random package.\n",
    "Unless otherwise stated, the scripts come from this project.\n",
    "I will :\n",
    "\n",
    "shuffle the Heritage Sites names\n",
    "shuffle the Region names\n",
    "synthesise random data for the number of visitors from integers that range from 0 - 553,348\n",
    "synthesise random data for price point variables for each demographic within the original scope, I will then ensure that 51 out of the 100 Sites have free entry / zero value\n",
    "Then merge these dataframes into one large dataset that mirrors the original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12042019 Investigate the DataSet\n",
    "#Experimenting with pandas functions\n",
    "#Adapted from\n",
    "# https://stackoverflow.com/questions/33034243/calculating-the-mean-and-std-on-excel-file-using-python\n",
    "#Import pandas module\n",
    "import pandas\n",
    "#\n",
    "#The standard deviation is amount of variability (or spread) \n",
    "#among the numbers in a data set, that is the standard (or typical) \n",
    "# amount of deviation (or distance) from the mean\n",
    "#https://wiki.kidzsearch.com/wiki/Standard_deviation\n",
    "\n",
    "#dataset = pandas.read_csv('irisdataset.txt')\n",
    "print(\" 'std' calculates and displays the standard deviation in each column\")\n",
    "print(df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a description of the output\n",
    "print (\"Data Visualisation - Countplot of OPW Regions\")\n",
    "#Code amended from https://stackoverflow.com/questions/42528921/how-to-prevent-overlapping-x-axis-labels-in-sns-countplot\n",
    "#Following on from the last plot, this countplot give a more accurate visualisation of the number of smokers versus non smokers.\n",
    "\n",
    "ax = sns.countplot(x=\"Region\", data=df)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a description of the output\n",
    "print (\"Data Visualisation - Countplot of OPW Counties\")\n",
    "#Code amended from https://amitkushwaha.co.in/data-visualization-part-1.html\n",
    "#Following on from the last plot, this countplot give a more accurate visualisation of the number of smokers versus non smokers.\n",
    "\n",
    "ax = sns.countplot(x=\"County\", data=df)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#sns.countplot('County', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Print a description of the output\n",
    "print (\"Data Visualisation - Countplot of 2015 Visitor Numbers\")\n",
    "#Code amended from https://stackoverflow.com/questions/42528921/how-to-prevent-overlapping-x-axis-labels-in-sns-countplot\n",
    "#Following on from the last plot, this countplot give a more accurate visualisation of the number of smokers versus non smokers.\n",
    "\n",
    "ax = sns.countplot(x=\"2015 Visitor Numbers\", data=df)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://seaborn.pydata.org/tutorial/distributions.html\n",
    "\n",
    "\n",
    "\n",
    "sns.jointplot(x=\"Region\", y=\"2015 Visitor Numbers\", data=df);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Comment** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code adapted from https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "ax = sns.boxplot(x=\"Region\", y=\"2015 Visitor Numbers\", data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesise/simulate a data set as closely matching the properties of the original as is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following on from the findings in Section 2, I will simulate a data set as closely matching their properties as possible using the numpy random package thus building on my [previous work](http://localhost:8888/?token=98bc2512905f44f91efe55dc0b350cacc78b93d3f4e55086) carried out during this course where I explored the numpy random package.<br>\n",
    "Unless otherwise stated, the scripts come from this project.<br>\n",
    "I will :\n",
    "- Permute the Heritage Sites names\n",
    "- Permute the Region names\n",
    "- Synthesise random data for the number of visitors from integers that range from 0 - 553,348\n",
    "- Synthesise random data for price point variables for each demographic within the original scope, I will then ensure that 51 out of the 100 Sites have free entry / zero value<br>\n",
    "- Synthesise random data for the number of cafes available at Heritage Sites<br>\n",
    "- Synthesise random data for the opening hours at Heritage sites<br>\n",
    "Then merge these dataframes into one large dataset that mirrors the original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shuffle the Heritage Site names and create a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a description of the output\n",
    "print (\"Synthesise a Dataset - Shuffle Heritage Site Names and print the array\")\n",
    "#code adapted from https://stackoverflow.com/questions/49545599/how-to-turn-a-pandas-column-into-array-and-transpose-it\n",
    "New_Names = df[['Name']]\n",
    "#Permute the synthesised dataframe. Permute is used because shuffle creates a \"key error\" when used with a dataframe.\n",
    "#df1 = pd.DataFrame(np.random.permutation(df[['Name']])\n",
    "New_Names = pd.DataFrame((np.random.permutation(New_Names)), columns = ['New_Names'])\n",
    "New_Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shuffle the OPW regions and create a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a description of the output\n",
    "print (\"Synthesise a Dataset - Permute Heritage Site Regions\")\n",
    "#code adapted from https://stackoverflow.com/questions/49545599/how-to-turn-a-pandas-column-into-array-and-transpose-it\n",
    "\n",
    "New_Regions = df[['Region']]\n",
    "df2 = pd.DataFrame((np.random.permutation(New_Regions)), columns = ['New_Region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "synthesise random data for the number of visitors from integers that range from 0 - 553348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a description of the output\n",
    "print (\"Synthesise a Dataset - Visitor Numbers and print the array\")\n",
    "#As per the numpy documentation, this command returns random integers from the “discrete uniform” distribution\n",
    "df3 = pd.DataFrame((np.random.randint(0, high=553348, size=100, dtype='l')), columns = ['New_Numbers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** Instead of forcing a nil amount of Visitors for 31 Heritage Site, I will allow the numpy library to generate data. My reason for this choice, is that the number of visitors in those sites was not zero - it was simply not collected for various business reasons e.g. the site is a main thoroughfare in the case of St. Stephen's Green. It will be more interesting dataset if these statistics are contained in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "synthesise random data for price point variables for each demographic within the original scope, I will then ensure that 51 out of the 100 Sites have free entry / zero value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a description of the output\n",
    "print (\"Synthesise a Dataset - Create random Adult Entrance Fees between the allowed range of values\")\n",
    "#Adult entrance price -  100 integers between 1 and 12 with a normal distribution\n",
    "#0-12 is not used because this would result in some random values of 0\n",
    "#Code adated from library documentation https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.random.html\n",
    "New_Adult = np.random.randint(1, high=12, size=100, dtype='l')\n",
    "New_Adult = pd.DataFrame(New_Adult)\n",
    "#Code adapted from https://stats.stackexchange.com/questions/283572/using-iloc-to-set-values/283575\n",
    "#Replace 51 values with free entry/zero\n",
    "New_Adult.loc[0:50,0] = 0\n",
    "#Permute the synthesised dataframe. Permute is used because shuffle creates a \"key error\" when used with a dataframe.\n",
    "df4 = pd.DataFrame(np.random.permutation(New_Adult), columns = ['New_Adult'])\n",
    "#df8 = pd.DataFrame((np.random.permutation(New_Family)), columns = ['New_Family']\n",
    "#df4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a description of the output\n",
    "print (\"Synthesise a Dataset - Create random values for the other Entrance Fees\")\n",
    "#Senior / Group entrance price - an integer between 1 and 9 with a normal distribution\n",
    "#Child entrance price - an integer between 1 and 7 with a normal distribution\n",
    "#Student entrance price - an integer between 1 and 8 with a normal distribution\n",
    "#Family entrance price - an integer between 1 and 32 with a normal distribution\n",
    "\n",
    "#Code adated from library documentation https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.random.html\n",
    "New_SnrGroup = np.random.randint(1, high=9, size=100, dtype='l')\n",
    "New_SnrGroup = pd.DataFrame(New_SnrGroup)\n",
    "New_SnrGroup.loc[0:50,0] = 0\n",
    "df5 = pd.DataFrame((np.random.permutation(New_SnrGroup)), columns = ['New_SnrGroup'])\n",
    "#\n",
    "New_Child = np.random.randint(1, high=7, size=100, dtype='l')\n",
    "New_Child = pd.DataFrame(New_Child)\n",
    "New_Child.loc[0:50,0] = 0\n",
    "df6 = pd.DataFrame((np.random.permutation(New_Child)), columns = ['New_Child'])\n",
    "#\n",
    "New_Student = np.random.randint(1, high=8, size=100, dtype='l')\n",
    "New_Student = pd.DataFrame(New_Student)\n",
    "New_Student.loc[0:50,0] = 0\n",
    "df7 = pd.DataFrame((np.random.permutation(New_Student)), columns = ['New_Student'])\n",
    "#\n",
    "New_Family = np.random.randint(1, high=32, size=100, dtype='l')\n",
    "New_Family = pd.DataFrame(New_Family)\n",
    "New_Family.loc[0:50,0] = 0\n",
    "df8 = pd.DataFrame((np.random.permutation(New_Family)), columns = ['New_Family'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adapted from https://stackoverflow.com/questions/28135436/concatenate-rows-of-two-dataframes-in-pandas\n",
    "New_Dataset = pd.concat([New_Names,df2, df3, df4, df5,df6, df7, df8], axis=1)\n",
    "New_Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References used in completing the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE RTE News, 2010. Galway respite\n",
    "funding will not be cut [Online].\n",
    "Available from : http://www.rte.ie/\n",
    "news/2010/0707/health.html [viewed 1\n",
    "February 2011].\n",
    "\n",
    "\n",
    "Assignment 2019 for Programming for Data Analysis module, GMIT. [Online] <br>\n",
    "Available on: https://github.com/ClodaghMurphy/Assignment-2019-progda [viewed 26 november 2019]<br>\n",
    "\n",
    "OPW OPEN DATA SETS   [Online]<br>\n",
    "Available on: https://www.opw.ie/en/opendata/#d.en.34620 [viewed 26 November 2019]<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
